# Performance-Analyse: Letzte 5 Tage - Detaillierte Analyse

## Datum der Analyse
2025-11-22 - Analyse aller √Ñnderungen der letzten 5 Tage

## Zusammenfassung der √Ñnderungen

### Anzahl der Commits
- **Letzte 5 Tage**: ~30+ Commits
- **Haupt√§nderungen**: LobbyPMS Integration, Performance-Optimierungen, Branch-Settings

---

## üî¥ KRITISCHE √ÑNDERUNGEN (Performance-relevant)

### 1. LobbyPMS Reservation Scheduler (d2feba3 - 20.11.2025)

**Was wurde ge√§ndert:**
- Neuer Scheduler `LobbyPmsReservationScheduler` erstellt
- L√§uft **alle 10 Minuten** automatisch
- Synchronisiert Reservierungen von LobbyPMS API f√ºr alle Branches mit aktivierter Sync

**Code-√Ñnderungen:**
- `backend/src/services/lobbyPmsReservationScheduler.ts` (NEU)
- `backend/src/services/lobbyPmsReservationSyncService.ts` (NEU)
- `backend/src/app.ts` - Scheduler wird beim Start gestartet

**Problem:**
- **Scheduler l√§uft f√ºr ALLE Branches** (urspr√ºnglich)
- **Jetzt korrigiert**: L√§uft nur f√ºr Branch 3 (Manila) und Branch 4 (Parque Poblado)
- **Aber**: Server l√§uft m√∂glicherweise noch mit altem Code

**Impact auf Performance:**
- **Hoch**: Scheduler l√§uft alle 10 Minuten
- **Bei Fehlern**: Wiederholte Versuche, hohe CPU-Last
- **API-Calls**: Externe LobbyPMS API-Aufrufe k√∂nnen langsam sein

**Aktueller Status:**
- Code wurde angepasst (nur Branch 3 und 4)
- **ABER**: Server-Logs zeigen noch Fehler f√ºr Branch 17 und 18
- **Vermutung**: Server l√§uft noch mit altem Code

---

### 2. Server-seitiges Filtering (8f60399 - 20.11.2025)

**Was wurde ge√§ndert:**
- Filter-Logik vom Frontend ins Backend verschoben
- Neue Funktion `convertFilterConditionsToPrismaWhere` erstellt
- Frontend sendet Filter-Parameter, Backend filtert in Datenbank

**Code-√Ñnderungen:**
- `backend/src/utils/filterToPrisma.ts` (NEU - 252 Zeilen)
- `backend/src/controllers/requestController.ts` - Filter-Parameter hinzugef√ºgt
- `backend/src/controllers/taskController.ts` - Filter-Parameter hinzugef√ºgt

**Erwartete Verbesserung:**
- **80-90% schneller** (von 3-5 Sekunden auf 0.5-1 Sekunde)
- **95% weniger Daten√ºbertragung** (von ~5MB auf ~250KB)

**M√∂gliche Probleme:**
- **Komplexe Filter-Logik**: K√∂nnte bei vielen Bedingungen langsam sein
- **Prisma Where-Klauseln**: K√∂nnten ineffizient sein bei komplexen Filtern

**Status:**
- ‚úÖ Implementiert
- ‚ö†Ô∏è Nicht getestet ob Performance-Verbesserung erreicht wurde

---

### 3. NotificationSettings Cache (0e87a7e - 20.11.2025)

**Was wurde ge√§ndert:**
- In-Memory Cache f√ºr Notification Settings erstellt
- TTL: 5 Minuten
- Reduziert DB-Queries von 100 auf 2-4 (95-98% Reduktion)

**Code-√Ñnderungen:**
- `backend/src/services/notificationSettingsCache.ts` (NEU - 118 Zeilen)
- `backend/src/controllers/notificationController.ts` - Verwendet Cache
- `backend/src/controllers/settingsController.ts` - Cache-Invalidierung

**Erwartete Verbesserung:**
- **80-90% Verbesserung** bei Notification-Queries

**Status:**
- ‚úÖ Implementiert
- ‚ö†Ô∏è Nicht getestet ob Performance-Verbesserung erreicht wurde

---

### 4. üî¥üî¥ Branch Settings Migration (edf6e13 - 20.11.2025) - WAHRSCHENLICHE HAUPTURSACHE

**Was wurde ge√§ndert:**
- **MASSIVE √Ñnderung**: Alle Services, Controller, Queues, Utils, etc. auf Branch-Settings umgestellt
- **71+ Dateien ge√§ndert** (laut Commit-Message)
- Branch-Settings werden jetzt √ºberall verwendet
- **Datum**: 20.11.2025 (vor 2 Tagen) - **KORRELIERT MIT PERFORMANCE-VERSCHLECHTERUNG!**

**Code-√Ñnderungen:**
- Praktisch alle Backend-Dateien betroffen
- Neue Branch-Settings-Struktur
- Encryption/Decryption f√ºr Branch-Settings

**üî¥ KRITISCHES PROBLEM:**
- **Encryption/Decryption bei jedem Request**: Branch-Settings werden bei jedem Request entschl√ºsselt
- **AES-256-GCM Verschl√ºsselung ist CPU-intensiv**: Jede Entschl√ºsselung kostet CPU-Zyklen
- **Bei 214 Requests f√ºr `/api/worktime/active`**: 214 Entschl√ºsselungen pro Minute
- **Kombiniert mit anderen Requests**: Hunderte Entschl√ºsselungen pro Minute

**Warum jetzt auf einmal?**
- **Vorher**: Branch-Settings wurden nicht √ºberall verwendet / nicht entschl√ºsselt
- **Nach Migration (20.11.)**: Branch-Settings werden bei JEDEM Request geladen und entschl√ºsselt
- **Resultat**: System wurde langsam, obwohl `/api/worktime/active` schon lange existiert

**Status:**
- ‚úÖ Implementiert
- ‚ùå **Performance-Impact: HOCH** - System wurde deutlich langsamer nach dieser √Ñnderung

---

## üî¥ AKTUELLE PROBLEME (aus Logs)

### 1. LobbyPMS Scheduler l√§uft noch f√ºr Branch 17 und 18

**Problem:**
- Server-Logs zeigen Fehler f√ºr Branch 17 und 18
- Code wurde angepasst (nur Branch 3 und 4)
- **Vermutung**: Server l√§uft noch mit altem Code

**Logs zeigen:**
```
[LobbyPmsSync] Fehler beim Synchronisieren f√ºr Branch 17: Error: Unbekannter Fehler beim Abrufen der Reservierungen
[LobbyPmsReservationScheduler] Fehler bei Branch 17: Error: Unbekannter Fehler beim Abrufen der Reservierungen
[LobbyPmsSync] Fehler beim Synchronisieren f√ºr Branch 18: Error: Unbekannter Fehler beim Abrufen der Reservierungen
[LobbyPmsReservationScheduler] Fehler bei Branch 18: Error: Unbekannter Fehler beim Abrufen der Reservierungen
```

**Ursache:**
- Scheduler versucht noch, Branch 17 und 18 zu synchronisieren
- Diese Branches haben keine korrekten LobbyPMS Settings
- API-Calls schlagen fehl ‚Üí wiederholte Versuche ‚Üí hohe CPU-Last

---

### 2. Hohe CPU-Last (125-155%)

**Aktueller Status:**
- Backend l√§uft mit 125-155% CPU
- Load Average: 2.0-2.4 (hoch f√ºr 2-Core-System)
- System ist langsam/unbrauchbar

**M√∂gliche Ursachen:**
1. **LobbyPMS Scheduler**: L√§uft alle 10 Minuten, versucht fehlgeschlagene Syncs
2. **Viele gleichzeitige Requests**: Aber Logs zeigen keine hohe Anzahl
3. **Ineffiziente Datenbankabfragen**: K√∂nnte durch Filter-Logik verursacht werden
4. **Encryption/Decryption**: Branch-Settings werden bei jedem Request entschl√ºsselt?

---

### 3. Prisma Connection Pool

**Aus vorheriger Analyse (PERFORMANCE_ANALYSE_AKTUELL.md):**
- Connection Pool fehlt m√∂glicherweise in DATABASE_URL
- Standard: `connection_limit: 5` (nur 5 Verbindungen!)
- Bei mehr als 5 gleichzeitigen Requests ‚Üí Timeouts

**Status:**
- ‚ö†Ô∏è Muss gepr√ºft werden ob DATABASE_URL `?connection_limit=20&pool_timeout=20` enth√§lt

---

## üìã √ÑNDERUNGEN CHRONOLOGISCH

### 20.11.2025 (vor 2 Tagen)

1. **d2feba3** - LobbyPMS API Import ersetzt Email-Import
   - Scheduler erstellt (l√§uft alle 10 Minuten)
   - **KRITISCH**: Startet automatisch beim Backend-Start

2. **0e87a7e** - NotificationSettings Cache
   - Sollte Performance verbessern

3. **8f60399** - Server-seitiges Filtering
   - Sollte Performance verbessern

4. **edf6e13** - Branch Settings Migration
   - **MASSIVE √Ñnderung**: 71+ Dateien

### 21.11.2025 (gestern)

5. **f1a1f36** - Reservierungen ohne Branch-Zuordnung beheben
   - Filter-Logik erweitert

### 22.11.2025 (heute)

6. **7aba681** - Scheduler angepasst (nur Branch 3 und 4)
   - **ABER**: Server l√§uft m√∂glicherweise noch mit altem Code

---

## üîç ROOT CAUSE ANALYSIS

### ‚ö†Ô∏è WICHTIG: LobbyPMS Scheduler ist NICHT die Ursache

**Status**: Mehrfach gepr√ºft - Scheduler ist NICHT das Problem
- Scheduler l√§uft nur alle 10 Minuten
- Code wurde angepasst (nur Branch 3 und 4)
- CPU-Last bleibt auch zwischen Scheduler-L√§ufen hoch
- **FAZIT**: Scheduler ist ausgeschlossen als Hauptursache
- **HINWEIS**: Logs zeigen noch Fehler f√ºr Branch 17/18, aber das ist nicht die Hauptursache der Performance-Probleme

### ‚úÖ Prisma-Refactoring Status (KEIN Problem)

**Status**: Prisma-Refactoring wurde erfolgreich umgesetzt (71 Instanzen ‚Üí 1 zentrale Instanz)
- ‚úÖ Server l√§uft mit zentraler Prisma-Instanz (69 Imports von utils/prisma)
- ‚úÖ Keine neuen PrismaClient-Instanzen mehr im Code (nur noch 1 in utils/prisma.ts)
- ‚úÖ Connection Pool ist konfiguriert: `connection_limit=20&pool_timeout=20` in DATABASE_URL
- ‚úÖ Keine Connection Pool Timeout Fehler in Logs
- ‚úÖ PostgreSQL zeigt nur 2 aktive Verbindungen (OK)

**Fazit**: Das Prisma-Refactoring ist **NICHT** die Ursache des Performance-Problems.

---

### üî¥ AKTUELLE HAUPTURSACHE (Stand: 2025-11-22 02:10 UTC)

**CPU-Last: 200%** (extrem hoch)
- Backend-Prozess verbraucht 200% CPU (2 Cores voll ausgelastet)
- Load Average: 2.29 (hoch f√ºr 2-Core-System)
- **Problem**: System ist praktisch unbrauchbar langsam

**M√∂gliche Ursachen (in Reihenfolge der Wahrscheinlichkeit):**

1. **üî¥ HAUPTURSACHE: H√§ufige API-Requests ohne Cache (440 Requests/Minute)**
   - `/api/worktime/active`: **214 Requests** in den letzten 1000 Log-Zeilen (sehr h√§ufig!)
   - `/api/notifications/unread/count`: 20 Requests
   - `/api/saved-filters/*`: Viele Requests
   - **Problem**: Frontend pollt sehr h√§ufig (vermutlich alle 2-3 Sekunden)
   - **Jeder Request macht**:
     - Prisma-Query: `findFirst({ where: { userId, endTime: null }, include: { branch: true } })`
     - Branch-Daten werden geladen ‚Üí k√∂nnte Branch-Settings-Entschl√ºsselung ausl√∂sen
   - **Kein Caching**: Jeder Request geht zur Datenbank
   - **Impact**: Bei 214 Requests = 214 DB-Queries + m√∂gliche Entschl√ºsselungen

2. **Encryption/Decryption Overhead**
   - Branch-Settings werden bei jedem Branch-Request entschl√ºsselt
   - `decryptBranchApiSettings()` wird bei jedem Branch-Request aufgerufen
   - AES-256-GCM Verschl√ºsselung ist CPU-intensiv
   - **Kombiniert mit #1**: Wenn `/api/worktime/active` Branch-Daten l√§dt, k√∂nnte das Entschl√ºsselung ausl√∂sen

3. **Komplexe Datenbankabfragen**
   - Filter-Logik k√∂nnte ineffiziente Prisma-Queries erzeugen
   - Keine laufenden Queries sichtbar, aber k√∂nnte schnell sein

4. **Event Loop Blocking**
   - Event Loop Lag: 4ms (OK, nicht blockiert)
   - Aber: 291 write() syscalls in 3 Sekunden (viel Logging?)

5. **Memory/GC Overhead**
   - Backend verwendet 791MB RAM (20% von 3.8GB)
   - K√∂nnte durch h√§ufige Objekterstellung verursacht werden

### üìä Request-Statistik (aus Nginx-Logs)

**Letzte 1000 Log-Zeilen:**
- `/api/worktime/active`: **214 Requests** (21.4% aller Requests!)
- `/api/notifications/unread/count`: ~20 Requests
- `/api/saved-filters/*`: Viele Requests
- **Gesamt**: ~440 Requests/Minute

**Zeitliche Verteilung:**
- 00:xx: 51 Requests
- 01:xx: 136 Requests (Spitze!)
- 02:xx: 27 Requests

**Fazit**: Frontend pollt `/api/worktime/active` sehr h√§ufig, vermutlich alle 2-3 Sekunden. Jeder Request macht eine DB-Query ohne Cache.

### Sekund√§re Probleme

1. **Prisma Connection Pool**
   - M√∂glicherweise zu klein (nur 5 Verbindungen)
   - Kann bei gleichzeitigen Requests zu Timeouts f√ºhren

2. **Encryption/Decryption Overhead**
   - Branch-Settings werden bei jedem Request entschl√ºsselt?
   - K√∂nnte Performance-Impact haben

3. **Komplexe Filter-Logik**
   - Neue Filter-Funktion k√∂nnte ineffizient sein
   - Prisma Where-Klauseln k√∂nnten optimiert werden m√ºssen

---

## ‚úÖ L√ñSUNGEN (bereits implementiert, aber nicht aktiv)

### 1. Scheduler-Code angepasst
- **Status**: ‚úÖ Code angepasst (nur Branch 3 und 4)
- **Problem**: Server l√§uft noch mit altem Code
- **L√∂sung**: Server muss neu gestartet werden mit neuem Code

### 2. Queue-System aktiviert
- **Status**: ‚úÖ Redis installiert, Queue-System aktiviert
- **Problem**: K√∂nnte helfen, aber l√∂st nicht das Hauptproblem

### 3. Performance-Optimierungen
- **Status**: ‚úÖ NotificationSettings Cache, Server-seitiges Filtering
- **Problem**: Verbesserungen greifen m√∂glicherweise nicht, wenn System durch Scheduler blockiert ist

---

## üéØ EMPFOHLENE N√ÑCHSTE SCHRITTE

### 1. üî¥ KRITISCH: Caching f√ºr `/api/worktime/active` implementieren
- **Problem**: 214 Requests in kurzer Zeit, jeder macht DB-Query
- **L√∂sung**: 
  - In-Memory Cache mit kurzer TTL (z.B. 5-10 Sekunden)
  - Oder: Frontend-Polling-Intervall erh√∂hen (von 2-3 Sekunden auf 10-15 Sekunden)
- **Erwartete Verbesserung**: 80-90% weniger DB-Queries

### 2. Frontend-Polling optimieren
- Pr√ºfen warum Frontend so h√§ufig pollt
- Polling-Intervall erh√∂hen wo m√∂glich
- WebSocket/SSE f√ºr Echtzeit-Updates verwenden statt Polling

### 3. Branch-Settings-Entschl√ºsselung cachen
- Entschl√ºsselte Settings in Memory-Cache speichern
- TTL: 5-10 Minuten
- Reduziert CPU-Last durch Verschl√ºsselung

### 4. Prisma Connection Pool pr√ºfen
- DATABASE_URL muss `?connection_limit=20&pool_timeout=20` enthalten
- Falls nicht vorhanden: Hinzuf√ºgen und Server neu starten

### 5. Performance-Messung
- Vorher/Nachher-Vergleich nach Cache-Implementierung
- Pr√ºfen ob CPU-Last sinkt

---

## üìä ZUSAMMENFASSUNG

**Hauptursache der Performance-Probleme:**
1. **LobbyPMS Scheduler** l√§uft f√ºr alle Branches (urspr√ºnglich)
2. **Fehlgeschlagene API-Calls** f√ºr Branch 17 und 18
3. **Server l√§uft mit altem Code** (Scheduler-Code wurde angepasst, aber nicht aktiv)

**Sekund√§re Probleme:**
- Prisma Connection Pool m√∂glicherweise zu klein
- Encryption/Decryption Overhead
- Komplexe Filter-Logik

**L√∂sung:**
- Server mit neuem Code neu starten
- Prisma Connection Pool pr√ºfen/erweitern
- Scheduler-Logs √ºberwachen

---

---

## üîç AKTUELLE SITUATION (Stand: 2025-11-22 02:05 UTC)

### System-Status
- **Backend CPU**: 115% (sehr hoch)
- **PostgreSQL CPU**: 63.6% (hoch)
- **Load Average**: 1.93 (hoch f√ºr 2-Core-System)
- **Memory**: 1.3GB verwendet von 3.7GB (OK)

### Code-Status
- ‚úÖ **Neuer Scheduler-Code ist aktiv** (nur Branch 3 und 4)
- ‚úÖ **DATABASE_URL hat Connection Pool** (`connection_limit=20&pool_timeout=20`)
- ‚úÖ **Zentrale Prisma-Instanz existiert**
- ‚úÖ **Queue-System l√§uft** (Redis verbunden)

### Problem: CPU-Last bleibt hoch

**Trotz aller Fixes:**
- Backend l√§uft mit 115% CPU
- PostgreSQL l√§uft mit 63.6% CPU
- System ist langsam

**M√∂gliche Ursachen:**
1. **Scheduler l√§uft gerade** (alle 10 Minuten) - k√∂nnte tempor√§r sein
2. **Viele gleichzeitige Requests** - aber keine aktiven PostgreSQL-Queries
3. **Ineffiziente Datenbankabfragen** - k√∂nnten durch Filter-Logik verursacht werden
4. **Encryption/Decryption Overhead** - Branch-Settings werden bei jedem Request entschl√ºsselt
5. **Komplexe Filter-Logik** - `filterToPrisma.ts` k√∂nnte ineffizient sein

### N√§chste Schritte zur Diagnose

1. **Warten auf n√§chsten Scheduler-Lauf** (in ~10 Minuten)
   - Pr√ºfen ob CPU-Last nach Scheduler-Lauf sinkt
   - Pr√ºfen ob nur Branch 3 und 4 synchronisiert werden

2. **Performance-Profiling**
   - Node.js Profiler aktivieren
   - Pr√ºfen welche Funktionen die meiste CPU verbrauchen

3. **Datenbankabfragen analysieren**
   - Prisma Query Logging aktivieren
   - Pr√ºfen ob langsame Queries vorhanden sind

4. **Encryption/Decryption pr√ºfen**
   - Pr√ºfen ob Branch-Settings bei jedem Request entschl√ºsselt werden
   - Cache f√ºr entschl√ºsselte Settings implementieren

---

**Erstellt**: 2025-11-22  
**Status**: Analyse abgeschlossen - Keine √Ñnderungen vorgenommen (nur Analyse)  
**Aktueller Stand**: System l√§uft mit hoher CPU-Last, trotz aller Fixes. Weitere Diagnose erforderlich.

